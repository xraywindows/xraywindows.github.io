<!doctype html>
<html class="no-js" lang="zh-CN">

<head>
        <link rel="canonical" href="https://xraywindows.github.io/news/article-100310.htm" />
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <title>深度学习自定义自动求导函数</title>
        <meta name="description" content="参考：链接1，链接2 官方示例： import torch from torch.autograd import Variable   class MyReLU(torch.autograd.Func" />
        <link rel="icon" href="/assets/website/img/xraywindows/favicon.ico" type="image/x-icon"/>

    <meta name="author" content="Xray Windows节点订阅官网">
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://xraywindows.github.io/news/article-100310.htm" />
    <meta property="og:site_name" content="Xray Windows节点订阅官网" />
    <meta property="og:title" content="深度学习自定义自动求导函数" />
    <meta property="og:image" content="https://xraywindows.github.io/uploads/20240815/ea60a8832a080ee2babfcf644d599534.webp" />
        <meta property="og:release_date" content="2025-04-20T09:04:35" />
    <meta property="og:updated_time" content="2025-04-20T09:04:35" />
        <meta property="og:description" content="参考：链接1，链接2 官方示例： import torch from torch.autograd import Variable   class MyReLU(torch.autograd.Func" />
        
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="manifest" href="site.webmanifest">

    <meta name="applicable-device" content="pc,mobile" />
    <meta name="renderer" content="webkit" />
    <meta name="force-rendering" content="webkit" />
    <meta http-equiv="Cache-Control" content="no-transform" />
    <meta name="robots" content="max-image-preview:large" />
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="深度学习自定义自动求导函数">
    <meta name="format-detection" content="telephone=no">

    <link rel="dns-prefetch" href="https:/www.googletagmanager.com">
    <link rel="dns-prefetch" href="https://www.googleadservices.com">
    <link rel="dns-prefetch" href="https://www.google-analytics.com">
    <link rel="dns-prefetch" href="https://pagead2.googlesyndication.com">
    <link rel="dns-prefetch" href="https://cm.g.doubleclick.net">
    
    <!-- CSS here -->
    <link rel="stylesheet" href="/assets/website/css/xraywindows/bootstrap.min.css">
    <link rel="stylesheet" href="/assets/website/css/xraywindows/owl.carousel.min.css">
    <link rel="stylesheet" href="/assets/website/css/xraywindows/slicknav.css">
    <link rel="stylesheet" href="/assets/website/css/xraywindows/flaticon.css">
    <link rel="stylesheet" href="/assets/website/css/xraywindows/animate.min.css">
    <link rel="stylesheet" href="/assets/website/css/xraywindows/magnific-popup.css">
    <link rel="stylesheet" href="/assets/website/css/xraywindows/fontawesome-all.min.css">
    <link rel="stylesheet" href="/assets/website/css/xraywindows/themify-icons.css">
    <link rel="stylesheet" href="/assets/website/css/xraywindows/slick.css">
    <link rel="stylesheet" href="/assets/website/css/xraywindows/nice-select.css">
    <link rel="stylesheet" href="/assets/website/css/xraywindows/style.css">
    <link rel="stylesheet" href="/assets/website/css/G.css" />
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3W6FX62N6S"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-3W6FX62N6S');
</script>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3332997411212854"
     crossorigin="anonymous"></script>
</head>

<body class="body-bg" data-page="detail">
        <!--? Preloader Start -->
    <div id="preloader-active">
        <div class="preloader d-flex align-items-center justify-content-center">
            <div class="preloader-inner position-relative">
                <div class="preloader-circle"></div>
                <div class="preloader-img pere-text">
                    <img src="/assets/website/img/xraywindows/logo/loder.jpg" alt="">
                </div>
            </div>
        </div>
    </div>
    <header>
        <!-- Header Start -->
        <div class="header-area">
            <div class="main-header ">
                <div class="header-bottom  header-sticky">
                    <div class="container">
                        <div class="row align-items-center">
                            <!-- Logo -->
                            <div class="col-md-4">
                                <div class="logo">
                                    <a href="/">
                                                                        <span>Xray Windows</span>
                                                                        </a>
                                </div>
                            </div>
                            <div class="col-md-8">
                                <div class="menu-wrapper  d-flex align-items-center justify-content-end">
                                    <!-- Main-menu -->
                                    <div class="main-menu d-none d-lg-block">
                                        <nav>
                                            <ul id="navigation">
                                                                                                <li><a href="/">首页</a></li>
                                                                                                <li><a href="/free-nodes/">免费节点</a></li>
                                                                                                <li><a href="/paid-subscribe/">推荐机场</a></li>
                                                                                                <li><a href="/client.htm">客户端</a></li>
                                                                                                <li><a href="/news/">新闻资讯</a></li>
                                                                                            </ul>
                                        </nav>
                                    </div>
                                </div>
                            </div>
                            <!-- Mobile Menu -->
                            <div class="col-12">
                                <div class="mobile_menu d-block d-lg-none"></div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Header End -->
    </header>
    <main>
        <!--? Hero Start -->
        <div class="slider-area2">
            <div class="slider-height2 hero-overly2 d-flex align-items-center">
                <div class="container">
                    <div class="row">
                        <div class="col-xl-12">
                            <div class="hero-cap hero-cap2 text-center">
                                <h1>深度学习自定义自动求导函数</h1>
                                <p>
                                    <a href="/">首页</a> / <a href="/news/">新闻资讯</a> / <span>正文</span>
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!--? Categories Area Start -->
        <div class="categories-area section-padding30">
            <div class="container">
                <div class="row">
                    <div class="col-md-9">
                                        <input type="hidden" id="share-website-info" data-name="" data-url="">
                <div class="xcblog-blog-detail">
                      				  				  				<div id="content_views" class="markdown_views prism-atom-one-dark"> <p>参考：<a href="http://www.m6000.cn/wp-content/themes/begin%20lts/inc/go.php?url=https://blog.csdn.net/Hungryof/article/details/78346304"  rel="nofollow">链接1</a>，<a href="http://www.m6000.cn/wp-content/themes/begin%20lts/inc/go.php?url=https://blog.csdn.net/tsq292978891/article/details/79364140"  rel="nofollow">链接2</a></p> <p>官方示例：</p> <pre><code class="prism language-python"><span class="token keyword">import</span> torch <span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable   <span class="token keyword">class</span> <span class="token class-name">MyReLU</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>Function<span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token triple-quoted-string string">"""     We can implement our own custom autograd Functions by subclassing     torch.autograd.Function and implementing the forward and backward passes     which operate on Tensors.     """</span>      @<span class="token builtin">staticmethod</span>     <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">:</span>         <span class="token triple-quoted-string string">"""         In the forward pass we receive a Tensor containing the input and return         a Tensor containing the output. ctx is a context object that can be used         to stash information for backward computation. You can cache arbitrary         objects for use in the backward pass using the ctx.save_for_backward method.          """</span>         ctx<span class="token punctuation">.</span>save_for_backward<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span> <span class="token comment"># ctx 用来保存反向求导所需要的数据,也就是可以在backward（）函数中使用的变量。</span>         <span class="token keyword">return</span> <span class="token builtin">input</span><span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token builtin">min</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>      @<span class="token builtin">staticmethod</span>     <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> grad_output<span class="token punctuation">)</span><span class="token punctuation">:</span>         <span class="token triple-quoted-string string">"""         In the backward pass we receive a Tensor containing the gradient of the loss         with respect to the output, and we need to compute the gradient of the loss         with respect to the input.         """</span>         <span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token operator">=</span> ctx<span class="token punctuation">.</span>saved_tensors         grad_input <span class="token operator">=</span> grad_output<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>         grad_input<span class="token punctuation">[</span><span class="token builtin">input</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>         <span class="token keyword">return</span> grad_input <span class="token comment">#反向传播求梯度，如果该参数为网络需要更新的参数，那么该梯度会被保存，方便之后的参数更新或者优化。</span>   dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor <span class="token comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span>  <span class="token comment"># N is batch size; D_in is input dimension;</span> <span class="token comment"># H is hidden dimension; D_out is output dimension.</span> N<span class="token punctuation">,</span> D_in<span class="token punctuation">,</span> H<span class="token punctuation">,</span> D_out <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">10</span>  <span class="token comment"># Create random Tensors to hold input and outputs, and wrap them in Variables.</span> x <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">,</span> D_in<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> y <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">,</span> D_out<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token comment"># Create random Tensors for weights, and wrap them in Variables.</span> w1 <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>D_in<span class="token punctuation">,</span> H<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> w2 <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>H<span class="token punctuation">,</span> D_out<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  learning_rate <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span> <span class="token keyword">for</span> t <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">500</span><span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token comment"># To apply our Function, we use Function.apply method. We alias this as 'relu'.</span>     relu <span class="token operator">=</span> MyReLU<span class="token punctuation">.</span><span class="token builtin">apply</span>      <span class="token comment"># Forward pass: compute predicted y using operations on Variables; we compute</span>     <span class="token comment"># ReLU using our custom autograd operation.</span>     y_pred <span class="token operator">=</span> relu<span class="token punctuation">(</span>x<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>w1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mm<span class="token punctuation">(</span>w2<span class="token punctuation">)</span>      <span class="token comment"># Compute and print loss</span>     loss <span class="token operator">=</span> <span class="token punctuation">(</span>y_pred <span class="token operator">-</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>     <span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token comment"># Use autograd to compute the backward pass.</span>     loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 反向传播会将可训练参数梯度保存。</span>      <span class="token comment"># Update weights using gradient descent</span>     w1<span class="token punctuation">.</span>data <span class="token operator">-=</span> learning_rate <span class="token operator">*</span> w1<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data     w2<span class="token punctuation">.</span>data <span class="token operator">-=</span> learning_rate <span class="token operator">*</span> w2<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data      <span class="token comment"># Manually zero the gradients after updating weights</span>     w1<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#梯度清零。</span>     w2<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>  </code></pre> <p>类继承：<br /><code>torch.autograd.Function</code>，只需要定义正向传播和反向传播函数，正向传播就是自己定义函数的计算方法；反向传播则是求导梯度，<code>ctx</code>这个东西就当做<code>self</code>来对待就行，可以用来存储反向求导要求的数据，比如正向传播的结果或者输入。</p> <h3> <a id="Linear__83" rel="nofollow"></a>自定义Linear 操作：</h3> <pre><code class="prism language-python"><span class="token keyword">import</span> torch <span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Function <span class="token keyword">import</span> warnings warnings<span class="token punctuation">.</span>filterwarnings<span class="token punctuation">(</span><span class="token string">"ignore"</span><span class="token punctuation">)</span>    <span class="token keyword">class</span> <span class="token class-name">LinearFunction1</span><span class="token punctuation">(</span>Function<span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token triple-quoted-string string">""" 描述：在pytorch中自定义一个操作，并定义它的梯度求法"""</span>     @<span class="token builtin">staticmethod</span>     <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>         ctx<span class="token punctuation">.</span>save_for_backward<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias<span class="token punctuation">)</span>   <span class="token comment"># shape: n,m,  m nout</span>         <span class="token comment"># ctx.needs_input_grad = (False,True,True)</span>         output <span class="token operator">=</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">)</span>  <span class="token comment"># n,m; m,c_out</span>         <span class="token keyword">if</span> bias <span class="token keyword">is</span> <span class="token operator">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>             output <span class="token operator">+=</span> bias<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>output<span class="token punctuation">)</span>             <span class="token comment"># output += torch.unsqueeze(bias,dim=0).expand_as(output)</span>             <span class="token comment"># output += bias   #广播。</span>         <span class="token comment"># ctx.save_for_backward(output)</span>         <span class="token keyword">return</span> output      @<span class="token builtin">staticmethod</span>     <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> grad_outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>         <span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias <span class="token operator">=</span> ctx<span class="token punctuation">.</span>saved_tensors         grad_input <span class="token operator">=</span> <span class="token boolean">None</span>         grad_weight <span class="token operator">=</span> <span class="token boolean">None</span>         grad_bias <span class="token operator">=</span> <span class="token boolean">None</span>         <span class="token keyword">if</span> ctx<span class="token punctuation">.</span>needs_input_grad<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">:</span>             grad_input <span class="token operator">=</span> grad_outputs @ <span class="token punctuation">(</span>weight<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>   <span class="token comment"># n,c_out;c_out,m</span>         <span class="token keyword">if</span> ctx<span class="token punctuation">.</span>needs_input_grad<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>             grad_weight <span class="token operator">=</span> <span class="token builtin">input</span><span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span> @ grad_outputs  <span class="token comment"># m,n    n,c_out</span>         <span class="token keyword">if</span> bias <span class="token keyword">is</span> <span class="token operator">not</span> <span class="token boolean">None</span> <span class="token operator">and</span> ctx<span class="token punctuation">.</span>needs_input_grad<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">:</span>              grad_bias <span class="token operator">=</span> grad_outputs<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>          <span class="token keyword">return</span> grad_input<span class="token punctuation">,</span>grad_weight<span class="token punctuation">,</span>grad_bias  <span class="token comment"># Inherit from Function</span> <span class="token keyword">class</span> <span class="token class-name">LinearFunction</span><span class="token punctuation">(</span>Function<span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token comment"># Note that both forward and backward are @staticmethods</span>     @<span class="token builtin">staticmethod</span>     <span class="token comment"># bias is an optional argument</span>     <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>         ctx<span class="token punctuation">.</span>save_for_backward<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias<span class="token punctuation">)</span>         output <span class="token operator">=</span> <span class="token builtin">input</span><span class="token punctuation">.</span>mm<span class="token punctuation">(</span>weight<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 20,20; 30,20 -&gt; 20,30</span>         <span class="token keyword">if</span> bias <span class="token keyword">is</span> <span class="token operator">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>             output <span class="token operator">+=</span> bias<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>output<span class="token punctuation">)</span>         <span class="token keyword">return</span> output      <span class="token comment"># This function has only a single output, so it gets only one gradient</span>     @<span class="token builtin">staticmethod</span>     <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> grad_output<span class="token punctuation">)</span><span class="token punctuation">:</span>         <span class="token comment"># This is a pattern that is very convenient - at the top of backward</span>         <span class="token comment"># unpack saved_tensors and initialize all gradients w.r.t. inputs to</span>         <span class="token comment"># None. Thanks to the fact that additional trailing Nones are</span>         <span class="token comment"># ignored, the return statement is simple even when the function has</span>         <span class="token comment"># optional inputs.</span>         <span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias <span class="token operator">=</span> ctx<span class="token punctuation">.</span>saved_tensors         grad_input <span class="token operator">=</span> grad_weight <span class="token operator">=</span> grad_bias <span class="token operator">=</span> <span class="token boolean">None</span>          <span class="token comment"># These needs_input_grad checks are optional and there only to</span>         <span class="token comment"># improve efficiency. If you want to make your code simpler, you can</span>         <span class="token comment"># skip them. Returning gradients for inputs that don't require it is</span>         <span class="token comment"># not an error.</span>         <span class="token keyword">if</span> ctx<span class="token punctuation">.</span>needs_input_grad<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">:</span>             grad_input <span class="token operator">=</span> grad_output<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>weight<span class="token punctuation">)</span>   <span class="token comment"># 20 30 , 30 20  -&gt; 20 20   或者 20 30 30 20</span>         <span class="token keyword">if</span> ctx<span class="token punctuation">.</span>needs_input_grad<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>             grad_weight <span class="token operator">=</span> grad_output<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mm<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>  <span class="token comment"># 30 20, 20 20 - &gt; 30 20</span>         <span class="token keyword">if</span> bias <span class="token keyword">is</span> <span class="token operator">not</span> <span class="token boolean">None</span> <span class="token operator">and</span> ctx<span class="token punctuation">.</span>needs_input_grad<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">:</span>             grad_bias <span class="token operator">=</span> grad_output<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>          <span class="token keyword">return</span> grad_input<span class="token punctuation">,</span> grad_weight<span class="token punctuation">,</span> grad_bias </code></pre> <p>也就是定义正向传播和反向传播并保存需要的数据到context中即可，函数内的数学运算可以不是pytorch支持的运算而只需要是python支持支持的即可（个人理解是这样）。</p> <p>测试操作是否正确：</p> <pre><code class="prism language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> gradcheck  linear <span class="token operator">=</span> LinearFunction<span class="token punctuation">.</span><span class="token builtin">apply</span>   <span class="token comment">#这里使用上边的为什么不行，去个别名。</span> <span class="token builtin">input</span> <span class="token operator">=</span> <span class="token punctuation">(</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>double<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">30</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>double<span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span> test <span class="token operator">=</span> gradcheck<span class="token punctuation">(</span>linear<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">,</span> atol<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span>test<span class="token punctuation">)</span>   linear <span class="token operator">=</span> LinearFunction1<span class="token punctuation">.</span><span class="token builtin">apply</span>   <span class="token comment">#这里使用上边的为什么不行，去个别名。</span> <span class="token builtin">input</span> <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>double<span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">30</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>double<span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span> test <span class="token operator">=</span> gradcheck<span class="token punctuation">(</span>linear<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">,</span> atol<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span>test<span class="token punctuation">)</span> </code></pre> <p>可以看到两种方都返回的是<code>True</code>,需要注意的是自己定义的操作输入的形状等问题而已。</p> <h4> <a id="_179" rel="nofollow"></a>用自己的操作来建立模型：</h4> <pre><code class="prism language-cpp">import torch<span class="token punctuation">.</span>nn as nn <span class="token keyword">class</span> <span class="token class-name">Linear</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token operator">:</span>     def <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_features<span class="token punctuation">,</span> output_features<span class="token punctuation">,</span> bias<span class="token operator">=</span>True<span class="token punctuation">)</span><span class="token operator">:</span>         <span class="token function">super</span><span class="token punctuation">(</span>Linear<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">__init__</span><span class="token punctuation">(</span><span class="token punctuation">)</span>          self<span class="token punctuation">.</span>input_features <span class="token operator">=</span> input_features         self<span class="token punctuation">.</span>output_features <span class="token operator">=</span> output_features          self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span><span class="token function">Parameter</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token function">randn</span><span class="token punctuation">(</span>input_features<span class="token punctuation">,</span> output_features<span class="token punctuation">)</span><span class="token punctuation">)</span>         <span class="token keyword">if</span> bias<span class="token operator">:</span>             self<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span><span class="token function">Parameter</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token function">randn</span><span class="token punctuation">(</span>output_features<span class="token punctuation">)</span><span class="token punctuation">)</span>         <span class="token keyword">else</span><span class="token operator">:</span>             self<span class="token punctuation">.</span><span class="token function">register_parameter</span><span class="token punctuation">(</span><span class="token string">"bias"</span><span class="token punctuation">,</span> None<span class="token punctuation">)</span>          <span class="token macro property"># self.weight.uniform(-0.1, 0.1)</span>         nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span><span class="token function">kaiming_uniform</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>         <span class="token keyword">if</span> bias<span class="token operator">:</span>             nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span><span class="token function">kaiming_uniform</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>bias<span class="token punctuation">)</span>      def <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token operator">:</span>         <span class="token keyword">return</span> <span class="token function">LinearFunction1</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bias<span class="token punctuation">)</span> # 调用自定义的操作。   </code></pre> <p>在自定义操作的基础上建立的<code>layer</code>就与其他<code>layer</code>一样都可以自动求导和优化参数了。</p> <h4> <a id="_207" rel="nofollow"></a>不可导情况：</h4> <p>上边的线性变换是的运算是可导的情况，也就是可以从输出一步步的用导数或者运算来表达，如果遇到那种不可导的情况，也就是无法显式的表达导数该怎么办？那就是自己制定导数求法，比如近似求导或者干脆用另一个黑盒函数来进行代替求导。那么既然是黑盒函数，那么反向的传播的时候函数中间的值的梯度什么的就很难进行计算了，这种问题就需要对<code>backward</code>函数进行稍微的改变：</p> <pre><code class="prism language-python"><span class="token triple-quoted-string string">"""当某个操作是不可导的，但是你却用了近似的方法来代替。"""</span> <span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>function <span class="token keyword">import</span> once_differentiable  <span class="token keyword">def</span> <span class="token function">un_differentibale_function</span><span class="token punctuation">(</span>grad_output<span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token string">"一些列不可导的神奇操作"</span>      grad_output_changed <span class="token operator">=</span> <span class="token boolean">None</span>      <span class="token keyword">return</span>  grad_output_changed   @<span class="token builtin">staticmethod</span> @once_differentiable <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> grad_output<span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>grad_output<span class="token punctuation">)</span><span class="token punctuation">)</span>     grad_output_changed <span class="token operator">=</span> un_differentibale_function<span class="token punctuation">(</span>grad_output<span class="token punctuation">)</span>     grad_input <span class="token operator">=</span> grad_output_changed     <span class="token keyword">return</span> grad_input </code></pre> <p><code>@once_differentiable</code> 神马意思，我也说不清，就当做隐式求导或者近似求导吧。</p> </p></div> 			                </div>
                <div class="clearfix"></div>
                <div class="col-md-12 mt-5">
                                        <p>上一个：<a href="/news/article-99545.htm">动物医生执照的报考条件要求（动物医生职业资格证书）</a></p>
                                        <p>下一个：<a href="/news/article-100311.htm">小型宠物粮食加工设备价格及图片大全（小型宠物粮食加工设备价格及图片大全及价格）</a></p>
                                    </div>
                                    </div>
                    <div class="col-md-3">
                        <div class="panel panel-default">
    <div class="panel-heading">
        <h3 class="panel-title">热门文章</h3>
    </div>
    <div class="panel-body">
        <ul class="p-0 x-0" style="list-style: none;margin: 0;padding: 0;">
                        <li class="py-2"><a href="/news/article-98747.htm" title="实体娃娃牌子排行(实体娃娃知名品牌)">实体娃娃牌子排行(实体娃娃知名品牌)</a></li>
                        <li class="py-2"><a href="/free-nodes/2025-3-8-winxray-github.htm" title="「3月8日」最高速度19.2M/S，2025年Xray每天更新免费机场订阅节点链接">「3月8日」最高速度19.2M/S，2025年Xray每天更新免费机场订阅节点链接</a></li>
                        <li class="py-2"><a href="/news/article-66583.htm" title="python虚拟环境virtualenv">python虚拟环境virtualenv</a></li>
                        <li class="py-2"><a href="/free-nodes/2025-4-17-free-node-subscribe-links.htm" title="「4月17日」最高速度22.9M/S，2025年Xray每天更新免费机场订阅节点链接">「4月17日」最高速度22.9M/S，2025年Xray每天更新免费机场订阅节点链接</a></li>
                        <li class="py-2"><a href="/free-nodes/2025-4-11-free-winxray-node.htm" title="「4月11日」最高速度19.9M/S，2025年Xray每天更新免费机场订阅节点链接">「4月11日」最高速度19.9M/S，2025年Xray每天更新免费机场订阅节点链接</a></li>
                        <li class="py-2"><a href="/free-nodes/2025-3-2-today-winxray-node.htm" title="「3月2日」最高速度22.3M/S，2025年Xray每天更新免费机场订阅节点链接">「3月2日」最高速度22.3M/S，2025年Xray每天更新免费机场订阅节点链接</a></li>
                        <li class="py-2"><a href="/news/article-69146.htm" title="广州哪里可以领养宠物猫咪（广州哪里可以领养宠物猫咪的）">广州哪里可以领养宠物猫咪（广州哪里可以领养宠物猫咪的）</a></li>
                        <li class="py-2"><a href="/news/article-77832.htm" title="中牧疫苗价格表（中牧疫苗销售电话）">中牧疫苗价格表（中牧疫苗销售电话）</a></li>
                        <li class="py-2"><a href="/news/article-80573.htm" title="全国宠物粮食基地在哪里（全国宠物粮食基地在哪里啊）">全国宠物粮食基地在哪里（全国宠物粮食基地在哪里啊）</a></li>
                        <li class="py-2"><a href="/free-nodes/2025-3-23-winxray-node-github.htm" title="「3月23日」最高速度22.8M/S，2025年Xray每天更新免费机场订阅节点链接">「3月23日」最高速度22.8M/S，2025年Xray每天更新免费机场订阅节点链接</a></li>
                    </ul>
    </div>
</div>

<div class="panel panel-default">
    <div class="panel-heading">
        <h3 class="panel-title">归纳</h3>
    </div>
    <div class="panel-body">
        <ul class="p-0 x-0" style="list-style: none;margin: 0;padding: 0;">
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">66</span> <a href="/date/2025-04/" title="2025-04 归档">2025-04</a></h4>
            </li>
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">90</span> <a href="/date/2025-03/" title="2025-03 归档">2025-03</a></h4>
            </li>
                    </ul>
    </div>
</div>

                    </div>
                </div>
            </div>
        </div>
        <!-- Services Area End -->
    </main>
        <footer>
        <!--? Footer Start-->
        <div class="footer-area section-bg" data-background="/assets/website/img/xraywindows/gallery/footer_bg.jpg">
            <div class="container">
                <div class="footer-bottom">
                    <div class="row d-flex justify-content-between align-items-center">
                        <div class="col-xl-9 col-lg-8">
                            <div class="footer-copy-right">
                                                    <p>
                                                <a href="/">首页</a> |
                                                <a href="/free-nodes/">免费节点</a> |
                                                <a href="/paid-subscribe/">推荐机场</a> |
                                                <a href="/client.htm">客户端</a> |
                                                <a href="/news/">新闻资讯</a> |
                                                <a href="/about-us.htm">关于我们</a> |
                        <a href="/disclaimer.htm">免责申明</a> |
                        <a href="/privacy.htm">隐私申明</a> |
                        <a href="/sitemap.xml">网站地图</a>
                    </p>
                                <p>
                                    Xray Windows节点订阅官网 版权所有 Powered by WordPress
                                </p>
                            </div>
                        </div>
                        <div class="col-xl-3 col-lg-4">
                            <!-- Footer Social -->
                            <div class="footer-social f-right">
                                <a href="#"><i class="fab fa-twitter"></i></a>
                                <a href="https://www.facebook.com/sai4ull"><i class="fab fa-facebook-f"></i></a>
                                <a href="#"><i class="fas fa-globe"></i></a>
                                <a href="#"><i class="fab fa-instagram"></i></a>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Footer End-->
    </footer>
    <!-- Scroll Up -->
    <div id="back-top">
        <a title="Go to Top" href="#"> <i class="fas fa-level-up-alt"></i></a>
    </div>
    <!-- JS here -->
    <script src="/assets/website/js/frontend/xraywindows/vendor/modernizr-3.5.0.min.js"></script>
    <!-- Jquery, Popper, Bootstrap -->
    <script src="/assets/website/js/frontend/xraywindows/vendor/jquery-3.5.1.min.js"></script>
    <script src="/assets/website/js/frontend/xraywindows/popper.min.js"></script>
    <script src="/assets/website/js/frontend/xraywindows/bootstrap.min.js"></script>
    <!-- Jquery Mobile Menu -->
    <script src="/assets/website/js/frontend/xraywindows/jquery.slicknav.min.js"></script>
    <!-- Jquery Slick , Owl-Carousel Plugins -->
    <script src="/assets/website/js/frontend/xraywindows/owl.carousel.min.js"></script>
    <script src="/assets/website/js/frontend/xraywindows/slick.min.js"></script>
    <!-- One Page, Animated-HeadLin -->
    <script src="/assets/website/js/frontend/xraywindows/wow.min.js"></script>
    <script src="/assets/website/js/frontend/xraywindows/animated.headline.js"></script>
    <script src="/assets/website/js/frontend/xraywindows/jquery.magnific-popup.js"></script>
    <!-- Nice-select, sticky -->
    <script src="/assets/website/js/frontend/xraywindows/jquery.nice-select.min.js"></script>
    <script src="/assets/website/js/frontend/xraywindows/jquery.sticky.js"></script>

    <!-- contact js -->
    <script src="/assets/website/js/frontend/xraywindows/contact.js"></script>
    <script src="/assets/website/js/frontend/xraywindows/jquery.form.js"></script>
    <script src="/assets/website/js/frontend/xraywindows/jquery.validate.min.js"></script>
    <script src="/assets/website/js/frontend/xraywindows/mail-script.js"></script>
    <script src="/assets/website/js/frontend/xraywindows/jquery.ajaxchimp.min.js"></script>
    <!-- Jquery Plugins, main Jquery -->
    <script src="/assets/website/js/frontend/xraywindows/plugins.js"></script>
    <script src="/assets/website/js/frontend/xraywindows/main.js"></script>
    <script src="https://www.freeclashnode.com/assets/js/frontend/invite-url.js"></script>
    <script src="/assets/website/js/frontend/G.js"></script>
</body>

</html>